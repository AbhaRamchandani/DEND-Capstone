{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "Aggregate and analyze US immigration data, by combining it with airport codes and city demographic.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "\n",
    "# Dataframe manipulation\n",
    "import pandas as pd \n",
    "# File system interaction\n",
    "import os\n",
    "import glob\n",
    "# Lookup state/city/immigration codes\n",
    "from codesAndAbbreviations import us_state_codes, state_udf, state_abrvtn, state_abrvtn_udf, city_codes, city_udf, immigration_codes, immigration_udf\n",
    "# For our ETL/ELT\n",
    "from pyspark.sql import SparkSession, SQLContext, GroupedData\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark session\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SQLContext\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this project, we will combine data 3 types of data:\n",
    "1. US Immigration Data (SAS)\n",
    "2. US City Demographics Data (csv)\n",
    "3. World Airport Data (csv)\n",
    "and create a data model that will enable analysis of immigration data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Used the following Udacity provided datasets - \n",
    "1. I94 Immigration Data: This data comes from the US National Tourism and Trade Office (https://travel.trade.gov/research/reports/i94/historical/2016.html). A data dictionary is included in the workspace. There's a sample file to set a sense of the data in csv format before reading it all in. The entire dataset will not be used.\n",
    "2. U.S. City Demographic Data: This data comes from OpenSoft (https://public.opendatasoft.com).\n",
    "3. Airport Code Table: This is a simple table of airport codes and corresponding cities (https://datahub.io/core/airport-codes#data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "\n",
    "# Creating dataframes\n",
    "df_demographics = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").load(\"us-cities-demographics.csv\")\n",
    "df_airport = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"airport-codes_csv.csv\")\n",
    "df_immigration = spark.read.format('com.github.saurfang.sas.spark').load(\"../../data/18-83510-I94-Data-2016/i94_oct16_sub.sas7bdat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(City='Silver Spring', State='Maryland', Median Age='33.8', Male Population='40601', Female Population='41862', Total Population='82463', Number of Veterans='1562', Foreign-born='30908', Average Household Size='2.6', State Code='MD', Race='Hispanic or Latino', Count='25924')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demographics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(ident='00A', type='heliport', name='Total Rf Heliport', elevation_ft='11', continent='NA', iso_country='US', iso_region='US-PA', municipality='Bensalem', gps_code='00A', iata_code=None, local_code='00A', coordinates='-74.93360137939453, 40.07080078125')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airport.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(cicid=8.0, i94yr=2016.0, i94mon=10.0, i94cit=111.0, i94res=111.0, i94port='CHM', arrdate=20728.0, i94mode=1.0, i94addr='HI', depdate=None, i94bir=70.0, i94visa=2.0, count=1.0, dtadfile='20161001', visapost=None, occup=None, entdepa='A', entdepd=None, entdepu=None, matflag=None, biryear=1946.0, dtaddto='12292016', gender='F', insnum='5245', airline='AF', admnum=67011135827.0, fltno='00342', visatype='WT')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3649136"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_immigration.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Data quality issues revolved around the following factors:\n",
    "* NULL values\n",
    "* Finding the right codes\n",
    "* Filtering the data we don't wish to report on\n",
    "* Performing aggregations\n",
    "\n",
    "#### Cleaning Steps\n",
    "Data quality issues nd the steps to clean up the data have been discussed #as we perform the cleanup in the next few steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "# 1st task: Clean up I94/immigration data\n",
    "# - remove nulls\n",
    "# - extract country of origin using immigration codes\n",
    "# - extract State names\n",
    "\n",
    "# We will use these udfs that we imported above\n",
    "# - state_abrvtn_udf, city_udf, immigration_udf\n",
    "# country_udf, abbrev_state_udf and city_code_udf were created with data from i94 SAS labels Descriptions file.\n",
    "\n",
    "immigration_data = df_immigration.filter(df_immigration.i94addr.isNotNull())\\\n",
    ".filter(df_immigration.i94res.isNotNull())\\\n",
    ".filter(col(\"i94addr\").isin(list(state_abrvtn.keys())))\\\n",
    ".filter(col(\"i94port\").isin(list(city_codes.keys())))\\\n",
    ".withColumn(\"origin_country\",immigration_udf(df_immigration[\"i94res\"]))\\\n",
    ".withColumn(\"dest_state_name\",state_abrvtn_udf(df_immigration[\"i94addr\"]))\\\n",
    ".withColumn(\"i94yr\",col(\"i94yr\").cast(\"integer\"))\\\n",
    ".withColumn(\"i94mon\",col(\"i94mon\").cast(\"integer\"))\\\n",
    ".withColumn(\"city_port_name\",city_udf(df_immigration[\"i94port\"]))\n",
    "\n",
    "imgrtn_data = immigration_data.select(\"cicid\",col(\"i94yr\").alias(\"year\"),col(\"i94mon\").alias(\"month\"),\\\n",
    "                             \"origin_country\",\"i94port\",\"city_port_name\",\\\n",
    "                             col(\"i94addr\").alias(\"state_code\"),\"dest_state_name\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+--------------+-------+------------------+----------+---------------+\n",
      "|cicid|year|month|origin_country|i94port|    city_port_name|state_code|dest_state_name|\n",
      "+-----+----+-----+--------------+-------+------------------+----------+---------------+\n",
      "|  8.0|2016|   10|        FRANCE|    CHM|CHAMPLAIN         |        HI|         Hawaii|\n",
      "|100.0|2016|   10|        TAIWAN|    AGA|AGANA             |        NY|       New York|\n",
      "|101.0|2016|   10|        TAIWAN|    AGA|AGANA             |        NY|       New York|\n",
      "|102.0|2016|   10|        TAIWAN|    AGA|AGANA             |        NY|       New York|\n",
      "|103.0|2016|   10|        TAIWAN|    AGA|AGANA             |        MA|  Massachusetts|\n",
      "+-----+----+-----+--------------+-------+------------------+----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imgrtn_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd task: Clean up demographic data\n",
    "# - represent the different factions of the population in percentages\n",
    "\n",
    "demographics_data = df_demographics\\\n",
    ".withColumn(\"Median Age\",col(\"Median Age\").cast(\"float\"))\\\n",
    ".withColumn(\"pct_male\",df_demographics[\"Male Population\"]/df_demographics[\"Total Population\"]*100)\\\n",
    ".withColumn(\"pct_female\",df_demographics[\"Female Population\"]/df_demographics[\"Total Population\"]*100)\\\n",
    ".withColumn(\"pct_veterans\",df_demographics[\"Number of Veterans\"]/df_demographics[\"Total Population\"]*100)\\\n",
    ".withColumn(\"pct_foreign_born\",df_demographics[\"Foreign-born\"]/df_demographics[\"Total Population\"]*100)\\\n",
    ".withColumn(\"pct_race\",df_demographics[\"Count\"]/df_demographics[\"Total Population\"]*100)\\\n",
    ".orderBy(\"State\")\n",
    "\n",
    "dmgrphcs_data = demographics_data.select(\"State\",col(\"State Code\").alias(\"state_code\"),\\\n",
    "                                 col(\"Median Age\").alias(\"median_age\"),\\\n",
    "                                 \"pct_male\",\"pct_female\",\"pct_veterans\",\\\n",
    "                                 \"pct_foreign_born\",\"Race\",\"pct_race\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+------------------+------------------+-----------------+------------------+--------------------+------------------+\n",
      "|  State|state_code|median_age|          pct_male|        pct_female|     pct_veterans|  pct_foreign_born|                Race|          pct_race|\n",
      "+-------+----------+----------+------------------+------------------+-----------------+------------------+--------------------+------------------+\n",
      "|Alabama|        AL|      38.1| 48.52311304292649| 51.47688695707351|8.797339171081992| 6.710767050562094|Black or African-...|32.552322937487446|\n",
      "|Alabama|        AL|      29.1| 48.09229392503407| 51.90770607496593|3.708637556183774| 4.785535601700258|  Hispanic or Latino| 2.516829709776485|\n",
      "|Alabama|        AL|      38.9|47.636815920398014|52.363184079601986|9.378701729447998| 2.515695332859512|               Asian|1.7398128405591091|\n",
      "|Alabama|        AL|      35.4| 47.15284217243477| 52.84715782756524|7.455654931052018|4.6548612565184015|               White|36.665071340970954|\n",
      "|Alabama|        AL|      38.9|47.636815920398014|52.363184079601986|9.378701729447998| 2.515695332859512|  Hispanic or Latino| 2.523098791755508|\n",
      "+-------+----------+----------+------------------+------------------+-----------------+------------------+--------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dmgrphcs_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd task: Aggregate demographic data\n",
    "# Based on \n",
    "# - Race\n",
    "# - State\n",
    "\n",
    "agg_dmgrphcs_data = dmgrphcs_data.groupBy(\"State\",\"state_code\",\"median_age\",\"pct_male\",\\\n",
    "                                    \"pct_female\",\"pct_veterans\",\\\n",
    "                                    \"pct_foreign_born\").pivot(\"Race\").avg(\"pct_race\")\n",
    "\n",
    "# Rename column headers\n",
    "agg_dmgrphcs_data = agg_dmgrphcs_data.select(\"State\",\"state_code\",\"median_age\",\"pct_male\",\"pct_female\",\"pct_veterans\",\"pct_foreign_born\",\\\n",
    "                                         col(\"American Indian and Alaska Native\").alias(\"Native_American\"),\\\n",
    "                                         col(\"Asian\"),col(\"Black or African-American\").alias(\"African\"),\\\n",
    "                                         col(\"Hispanic or Latino\").alias(\"South_American\"),\"White\")\n",
    "\n",
    "\n",
    "# Find average numbers of different population segments by state\n",
    "avg_dmgrphcs_data = agg_dmgrphcs_data.groupBy(\"State\",\"state_code\").avg(\"median_age\",\"pct_male\",\"pct_female\",\\\n",
    "                                                       \"pct_veterans\",\"pct_foreign_born\",\"Native_American\",\\\n",
    "                                                       \"Asian\",\"African\",\"South_American\",\"White\").orderBy(\"State\")\n",
    "\n",
    "# Data selection and rounding all the numbers to make them readable\n",
    "avg_dmgrphcs_data = avg_dmgrphcs_data.select(\"State\",\"state_code\",round(col(\"avg(median_age)\"),2).alias(\"median_age\"),\\\n",
    "                  round(col(\"avg(pct_male)\"),2).alias(\"pct_male\"),\\\n",
    "                  round(col(\"avg(pct_female)\"),2).alias(\"pct_female\"),\\\n",
    "                  round(col(\"avg(pct_veterans)\"),2).alias(\"pct_veterans\"),\\\n",
    "                  round(col(\"avg(pct_foreign_born)\"),2).alias(\"pct_foreign_born\"),\\\n",
    "                  round(col(\"avg(Native_American)\"),2).alias(\"Native_American\"),\\\n",
    "                  round(col(\"avg(Asian)\"),2).alias(\"Asian\"),\\\n",
    "                  round(col(\"avg(South_American)\"),2).alias(\"South_American\"),\\\n",
    "                  round(col(\"avg(African)\"),2).alias(\"African\"),\\\n",
    "                  round(col('avg(White)'),2).alias('White')\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+--------+----------+------------+----------------+---------------+-----+--------------+-------+-----+\n",
      "|     State|state_code|median_age|pct_male|pct_female|pct_veterans|pct_foreign_born|Native_American|Asian|South_American|African|White|\n",
      "+----------+----------+----------+--------+----------+------------+----------------+---------------+-----+--------------+-------+-----+\n",
      "|   Alabama|        AL|     36.23|   47.25|     52.75|        6.76|            5.13|           0.81| 2.91|          3.57|  45.01|52.04|\n",
      "|    Alaska|        AK|      32.2|    51.2|      48.8|         9.2|           11.13|          12.17|12.33|          9.13|   7.74|71.21|\n",
      "|   Arizona|        AZ|     35.04|   48.81|     51.19|        6.61|           12.64|           2.82| 5.13|         28.77|   6.01|82.68|\n",
      "|  Arkansas|        AR|     32.77|   48.41|     51.59|        5.21|           10.72|           1.83|  4.1|         14.17|  21.85|68.03|\n",
      "|California|        CA|     36.18|   49.36|     50.64|        4.13|           27.57|           1.67|17.93|         37.81|   7.45|62.67|\n",
      "+----------+----------+----------+--------+----------+------------+----------------+---------------+-----+--------------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg_dmgrphcs_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4th task: Cleanup Airport data\n",
    "# Cleanup involves\n",
    "# - Adding State codes\n",
    "# - Filtering small airports in the US\n",
    "\n",
    "airport_data = df_airport.filter(df_airport[\"type\"]==\"small_airport\")\\\n",
    ".filter(df_airport[\"iso_country\"]==\"US\")\\\n",
    ".withColumn(\"iso_region\",substring(df_airport[\"iso_region\"],4,2))\\\n",
    ".withColumn(\"elevation_ft\",col(\"elevation_ft\").cast(\"float\"))\n",
    "\n",
    "# Calculate avaerge elevations\n",
    "avg_elevation = airport_data.groupBy(\"iso_country\",\"iso_region\").avg(\"elevation_ft\")\n",
    "\n",
    "# View non-duplicate data\n",
    "arprt_data = avg_elevation.select(col(\"iso_country\").alias(\"country\"),\\\n",
    "                                               col(\"iso_region\").alias(\"state\"),\\\n",
    "                                               round(col(\"avg(elevation_ft)\"),2).alias(\"avg_elevation\")).orderBy(\"iso_region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+-------------+\n",
      "|country|state|avg_elevation|\n",
      "+-------+-----+-------------+\n",
      "|     US|   AK|       545.07|\n",
      "|     US|   AL|        414.6|\n",
      "|     US|   AR|       488.45|\n",
      "|     US|   AZ|      3098.01|\n",
      "|     US|   CA|      1261.37|\n",
      "+-------+-----+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "arprt_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "We will implement the standard Star schema model widely used for Data Analytics\n",
    "1. Dimension Tables\n",
    "   - airport\n",
    "     Columns: country, state, avg_elevation\n",
    "   - demographics\n",
    "     Columns: State, state_code, median_age, pct_male, pct_female, pct_veterans, pct_foreign_born, Native_American, Asian, South_American, African, White\n",
    "   - i94_immigration\n",
    "     Columns: cicid, year, month, origin_country, i94port, city_port_us, state, dest_state_us\n",
    "2. Fact Table\n",
    "   - immigration_fact\n",
    "     Columns: year, imm_month, imm_origin, imm_state, 'to_imm_state_count', 'avg_elevation', 'pct_foreign_born', 'Native_American', 'Asian', 'South_American', 'African', 'White'\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "Steps to create data pipeline:\n",
    "1. Dimention tables will be loaded first using the clean data, extracted above.\n",
    "2. Create Fact table..\n",
    "   a. using SQL query that joins it to the dimension tables.\n",
    "   b. convert the Fact table to a spark dataframe.\n",
    "   c. use the spark data frame to write the data to a parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write code here\n",
    "\n",
    "# Dimension tables\n",
    "\n",
    "imgrtn_data.createOrReplaceTempView(\"immigration\")\n",
    "avg_dmgrphcs_data.createOrReplaceTempView(\"demographics\")\n",
    "arprt_data.createOrReplaceTempView(\"airport\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since fact table is large, we need the ETL to not timeout...\n",
    "\n",
    "sqlContext.setConf(\"spark.sql.autoBroadcastJoinThreshold\", \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fact table: Immigration data by State\n",
    "\n",
    "immigration_by_states = spark.sql(\"\"\"SELECT \n",
    "                                    i.year,\n",
    "                                    i.month AS imm_month,\n",
    "                                    i.origin_country AS imm_origin,\n",
    "                                    i.dest_state_name AS to_imm_state,\n",
    "                                    COUNT(i.state_code) AS to_imm_state_count,\n",
    "                                    a.avg_elevation,\n",
    "                                    d.pct_foreign_born,\n",
    "                                    d.Native_American,\n",
    "                                    d.Asian,\n",
    "                                    d.South_American,\n",
    "                                    d.African,\n",
    "                                    d.White\n",
    "                                    FROM immigration i JOIN demographics d ON i.state_code=d.state_code\n",
    "                                    JOIN airport a ON a.state=d.state_code\n",
    "                                    GROUP BY i.year,i.month, i.origin_country,\\\n",
    "                                    i.dest_state_name,i.state_code,a.avg_elevation,\\\n",
    "                                    d.pct_foreign_born,d.Native_American,\\\n",
    "                                    d.Asian,d.South_American,\\\n",
    "                                    d.White,d.African\n",
    "                                    ORDER BY i.origin_country,i.state_code\n",
    "                                    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+-----------+------------+------------------+-------------+----------------+---------------+-----+--------------+-------+-----+\n",
      "|year|imm_month| imm_origin|to_imm_state|to_imm_state_count|avg_elevation|pct_foreign_born|Native_American|Asian|South_American|African|White|\n",
      "+----+---------+-----------+------------+------------------+-------------+----------------+---------------+-----+--------------+-------+-----+\n",
      "|2016|       10|AFGHANISTAN|     Arizona|                 3|      3098.01|           12.64|           2.82| 5.13|         28.77|   6.01|82.68|\n",
      "|2016|       10|AFGHANISTAN|  California|                35|      1261.37|           27.57|           1.67|17.93|         37.81|   7.45|62.67|\n",
      "|2016|       10|AFGHANISTAN|    Colorado|                 1|      5912.82|            9.58|           2.04| 4.93|          22.2|   4.21|87.95|\n",
      "|2016|       10|AFGHANISTAN| Connecticut|                 3|       490.29|           25.21|           1.26| 5.34|         34.81|  24.28|59.61|\n",
      "|2016|       10|AFGHANISTAN|     Florida|                20|        77.67|           24.92|           0.93| 3.97|         28.42|  23.54|70.39|\n",
      "+----+---------+-----------+------------+------------------+-------------+----------------+---------------+-----+--------------+-------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting SQL table to Spark DF...\n",
    "\n",
    "immigration_by_states.toDF('year', 'imm_month', 'imm_origin', 'to_imm_state', \\\n",
    "          'to_imm_state_count', 'avg_elevation',\\\n",
    "          'pct_foreign_born', 'Native_American', 'Asian', 'South_American', 'African', 'White').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write Fact table to parquet\n",
    "\n",
    "immigration_by_states.write.parquet(\"immigration_by_states\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Running quality checks include checking for NULL values and target table record count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------------------+----------------------+\n",
      "|(year IS NULL)|(imm_month IS NULL)|(to_imm_state IS NULL)|\n",
      "+--------------+-------------------+----------------------+\n",
      "|         false|              false|                 false|\n",
      "+--------------+-------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "\n",
    "# Checking for NULL values in year, month, to_immig_state\n",
    "\n",
    "immigration_by_states.select(isnull('year'),\\\n",
    "                             isnull('imm_month'),\\\n",
    "                             isnull('to_imm_state')).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|sum(to_imm_state_count)|\n",
      "+-----------------------+\n",
      "|                3330304|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking if the target fact table is even loaded by counting the no. of records in it.\n",
    "\n",
    "immigration_by_states.select(sum('to_imm_state_count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data dictionary \n",
    "Create a data dictionary for your data model. For each field, provide a brief description of what the data is and where it came from. You can include the data dictionary in the notebook or in a separate file.\n",
    "\n",
    "\n",
    "##### Please check README.md to find the details about the Data Dictionary!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "##### Answer: Apache Spark / PySpark module in Python has been used to perform the ETL for this project. I tried using Dataframes alone at first. With Dataframes, it is very easy and quick to process the dimension tables / smaller data. As we move to millions of records / our Fact table, theprocessing speed reduces a lot. \n",
    "* Propose how often the data should be updated and why.\n",
    "##### Answer: Monthly, typically immigration services makes very many decisions evry month and their decisions/strategies remain the same for that whole month.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    "##### Answer: I will move up to Hadoop distributed systems to allow better parallel processing. \n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "##### Answer: Airflow is the best solution in this case to manage and schedule data pipelines.\n",
    " * The database needed to be accessed by 100+ people.\n",
    "##### Answer: This is generally a sensitive data, ideally should not be accessed by everyone. So, I would create a Web App solution that can only be accessed by the government employees.\n",
    " \n",
    " \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
